{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib\n",
    "#import urllib2\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "#import cv2\n",
    "import h5py\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "import random as rand\n",
    "from operator import add\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "import tarfile\n",
    "import urllib\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "#import cPickle as pickle\n",
    "import scipy.io as sio\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "import re\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#import nltk.data\n",
    "from sklearn.utils import shuffle\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_question(question):\n",
    "    \"\"\"remove non-letters, return a list of words\"\"\"\n",
    "    if type(question) != float:\n",
    "        question =  re.sub('[^a-zA-Z]', \" \", question)\n",
    "        return question.lower().split()\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_questions(questions):\n",
    "    \"\"\"clean questions that are not empty, return a list of lists of words\"\"\"\n",
    "    new_questions = []   \n",
    "        \n",
    "    for idx, question in enumerate(questions):        \n",
    "        #if (idx+1)%100000 == 0:\n",
    "            #print(\"Review %d of %d\\n\" % ( idx+1, len(questions) ))\n",
    "        \n",
    "        new_questions.append(clean_question(question))\n",
    "\n",
    "    return new_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_y = train_full['is_duplicate']\n",
    "train_full_q1 = clean_questions(train_full['question1'])\n",
    "train_full_q2 = clean_questions(train_full['question2'])\n",
    "test_q1 = clean_questions(test['question1'])\n",
    "test_q2 = clean_questions(test['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404290 404290 404290\n",
      "match records:  0.369197853026293\n",
      "non_match records:  0.6308021469737071\n",
      "2345796 2345796\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(len(train_full_q1),len(train_full_q2), len(train_full_y))\n",
    "print(\"match records: \", np.count_nonzero(train_full_y)/len(train_full_y) )\n",
    "print(\"non_match records: \", 1-np.count_nonzero(train_full_y)/len(train_full_y) )\n",
    "print(len(test_q1), len(test_q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the questions' length\n",
      "max: 246 ; 95%:  23.0 ; 92%:  20.0 ; 90%:  18.0 ; 80%:  14.0 ; median:  10.0\n"
     ]
    }
   ],
   "source": [
    "# distribution of the questions' length\n",
    "questions_len = []\n",
    "for q in train_full_q1 + train_full_q2 + test_q1 + test_q2:\n",
    "    questions_len.append(len(q))\n",
    "\n",
    "print(\"Distribution of the questions' length\")\n",
    "\n",
    "print(\"max:\", np.amax(np.array(questions_len)), \n",
    "      \"; 95%: \", np.percentile(np.array(questions_len), 95),\n",
    "      \"; 92%: \", np.percentile(np.array(questions_len), 92),\n",
    "      \"; 90%: \", np.percentile(np.array(questions_len), 90),\n",
    "      \"; 80%: \", np.percentile(np.array(questions_len), 80),\n",
    "      \"; median: \", np.percentile(np.array(questions_len), 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    return list(set(chain.from_iterable([word.lemma_names() for word in synonyms])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synonyms(\"you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def augment_question(question, k):\n",
    "    words_to_change = random.sample(question, int(k*len(question)))\n",
    "    for idx, word in enumerate(question):\n",
    "        if word in words_to_change:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms != []:\n",
    "                synonym = random.choice(synonyms)\n",
    "                question[idx] = synonym\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'can', 'i', 'convert', 'raw', 'files', 'to', 'jpeg', 'in', 'photos', 'in', 'a', 'macbook']\n"
     ]
    }
   ],
   "source": [
    "q1 = train_full_q1[1000]\n",
    "print(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1a = augment_question(q1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'sleep_with', 'you', 'please', 'help']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment_question(['i', 'love', 'you', 'please', 'help'], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_dataset(dataset):\n",
    "    if type(dataset[0][1]) == str:\n",
    "        for question in dataset:\n",
    "            new_question = augment_question(question, 0.5)            \n",
    "            dataset.append(new_question)   \n",
    "        return dataset\n",
    "    else:\n",
    "        return dataset+ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-846f429d375e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maugment_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_full_q1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-8ea706ddaaf3>\u001b[0m in \u001b[0;36maugment_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maugment_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mnew_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-7900675273ef>\u001b[0m in \u001b[0;36maugment_question\u001b[0;34m(question, k)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_to_change\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0msynonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msynonyms\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0msynonym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-7c03d0b2aef9>\u001b[0m in \u001b[0;36mget_synonyms\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msynonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynonyms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "augment_dataset(train_full_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in [train_full_q1, train_full_q2, train_full_y]:\n",
    "    dataset = augment_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_q1, cv_q1, train_q2, cv_q2, train_y, cv_y = train_test_split(train_full_q1, train_full_q2, train_full_y, test_size = 0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_q1, train_q2, train_y = shuffle(train_q1, train_q2, train_y, random_state=3)\n",
    "cv_q1, cv_q2, cv_y = shuffle(cv_q1, cv_q2, cv_y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323432 323432 323432 80858 80858 80858\n"
     ]
    }
   ],
   "source": [
    "print(len(train_q1), len(train_q2), len(train_y), len(cv_q1), len(cv_q2), len(cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data/train_q1', train_q1)\n",
    "np.save('data/train_q2', train_q2)\n",
    "np.save('data/train_y', train_y)\n",
    "np.save('data/cv_q1', cv_q1)\n",
    "np.save('data/cv_q2', cv_q2)\n",
    "np.save('data/cv_y', cv_y)\n",
    "np.save('data/test_q1', test_q1)\n",
    "np.save('data/test_q2', test_q2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
