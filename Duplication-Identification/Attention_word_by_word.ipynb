{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation\n",
    "\n",
    "X: input, left question + one unknown thing + right question\n",
    "length N = len_l  + 1 + len_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import zipfile\n",
    "import urllib\n",
    "import pickle\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.callbacks import *\n",
    "# from visualizer import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils.np_utils import to_categorical, accuracy\n",
    "from keras.layers.core import *\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge, TimeDistributed\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reader import *\n",
    "from myutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_i(i):  # get element i\n",
    "    return X[:,i,:]\n",
    "\n",
    "def get_Y(X):\n",
    "    return X[:, :xmaxlen, :]  # get first xmaxlen elem from time dim\n",
    "\n",
    "def get_H(X):\n",
    "    xmaxlen=K.params['xmaxlen']\n",
    "    return X[:, xmaxlen:, :] # get elements L+1 to N\n",
    "\n",
    "def get_H_n(X):\n",
    "    ans = X[:, -1, :]  # get last element from time dim\n",
    "    return ans\n",
    "\n",
    "def get_R(X):\n",
    "    Y, alpha = X[0], X[1]\n",
    "    ans = K.T.batched_dot(Y, alpha)\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_attention(sentence_length=20, verbose=False):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1509.06664\n",
    "    Reasoning about Entailment with Neural Attention\n",
    "    Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiský, Phil Blunsom\n",
    "    Submitted on 22 Sep 2015 (v1), last revised 1 Mar 2016\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    inputs = Input(shape=(sentence_length*2+1, 300), name='inputs')\n",
    "    #x = Embedding(output_dim=opts.emb, input_dim=opts.max_features, input_length=N, name='x')(main_input)\n",
    "    drop_out = Dropout(0.1, name='dropout')(inputs)\n",
    "    \n",
    "    lstm_fwd = LSTM(150, return_sequences=True, name='lstm_fwd')(drop_out)\n",
    "    lstm_bwd = LSTM(150, return_sequences=True, go_backwards=True, name='lstm_bwd')(drop_out)\n",
    "    \n",
    "    bilstm = layers.concatenate([lstm_fwd, lstm_bwd], name='bilstm')\n",
    "    drop_out = Dropout(0.1)(bilstm)\n",
    "    \n",
    "    ###################################################\n",
    "    # Attention \n",
    "    # M = tanh(Wy*Y + (Wh*hN)*eL)\n",
    "    Y = Lambda(get_Y, name=\"Y\", output_shape=(sentence_length, 300))(drop_out)\n",
    "    W_Y = TimeDistributed(Dense(k, W_regularizer=l2(0.01)), name=\"weight_Y\")(Y)\n",
    "    \n",
    "    hN = Lambda(get_H_n, output_shape=(300,), name=\"hN\")(drop_out)   \n",
    "    W_hN = Dense(k, W_regularizer=l2(0.01), name=\"W_hN\")(hN)\n",
    "    W_hN_eL = RepeatVector(L, name=\"W_hN_eL\")(W_hN)   \n",
    "    \n",
    "    merged = layers.add([W_Y, W_hN_eL], name=\"merged\")\n",
    "    M = Activation('tanh', name=\"M\")(merged)\n",
    "    \n",
    "    # alpha = softmax(wT * Mt)\n",
    "    alpha_ = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_\")(M)\n",
    "    flat_alpha = Flatten(name=\"flat_alpha\")(alpha_)\n",
    "    alpha = Dense(L, activation='softmax', name=\"alpha\")(flat_alpha)\n",
    "    \n",
    "    # r = Y*alphaT\n",
    "    Y_trans = Permute((2, 1), name=\"Y_trans\")(Y)  # of shape (None,300,20)\n",
    "    r_ = merge([Y_trans, alpha], output_shape=(k, 1), name=\"r_\", mode=get_R)\n",
    "    r = Reshape((k,), name=\"r\")(r_)\n",
    "    \n",
    "    # h* = tanh (Wp*r + Wx * hN)\n",
    "    W_r = Dense(k, W_regularizer=l2(0.01))(r)\n",
    "    W_hN = Dense(k, W_regularizer=l2(0.01))(hN)\n",
    "    merged = layers.add([W_r, W_hN])    \n",
    "    h_star = Activation('tanh')(merged)    \n",
    "    \n",
    "    ####################################\n",
    "    #Output\n",
    "    output  = Dense(3, activation='softmax')(h_star)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    # plot(model, 'model.png')\n",
    "    # # model.compile(loss={'output':'binary_crossentropy'}, optimizer=Adam())\n",
    "    # model.compile(loss={'output':'categorical_crossentropy'}, optimizer=Adam(options.lr))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(options.lr))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_attention_wordbyword(opts, verbose=False):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1509.06664\n",
    "    Reasoning about Entailment with Neural Attention\n",
    "    Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiský, Phil Blunsom\n",
    "    Submitted on 22 Sep 2015 (v1), last revised 1 Mar 2016    \n",
    "    \"\"\"\n",
    "    \n",
    "    k = 2 * opts.lstm_units  # 300\n",
    "    L = opts.xmaxlen  # 20\n",
    "    N = opts.xmaxlen + opts.ymaxlen + 1  # for delim\n",
    "    print \"x len\", L, \"total len\", N\n",
    "    print \"k\", k, \"L\", L\n",
    "\n",
    "    inputs = Input(shape=(N, 300), name='inputs')\n",
    "    #x = Embedding(output_dim=opts.emb, input_dim=opts.max_features, input_length=N, name='x')(main_input)\n",
    "    drop_out = Dropout(0.1, name='dropout')(inputs)\n",
    "    \n",
    "    lstm_fwd = LSTM(opts.lstm_units, return_sequences=True, name='lstm_fwd')(drop_out)\n",
    "    lstm_bwd = LSTM(opts.lstm_units, return_sequences=True, go_backwards=True, name='lstm_bwd')(drop_out)\n",
    "    \n",
    "    bilstm = merge([lstm_fwd, lstm_bwd], name='bilstm', mode='concat')\n",
    "    drop_out = Dropout(0.1)(bilstm)\n",
    "    \n",
    "    ###################################################\n",
    "    # Word_by_Word Attention \n",
    "    # Mt = tanh(Wy*Y + (Wh*ht + Wr*rt-1)*eL) \n",
    "    # alpha_t = softmax(wT * Mt)\n",
    "    # rt = Y*alphaT + tanh(Wt*rt-1)\n",
    "\n",
    "    \n",
    "    # 1) M1 = tanh(Wy*Y + Wh*h1*eL)    \n",
    "    ## Wy*Y\n",
    "    Y = Lambda(get_Y, arguments={\"xmaxlen\": L}, name=\"Y\", output_shape=(L, k))(drop_out)\n",
    "    W_Y = TimeDistributed(Dense(k, W_regularizer=l2(0.01)), name=\"W_Y\")(Y)\n",
    "    \n",
    "    ##  Wh*h1*eL\n",
    "    H = Lambda(get_H, output_shape=(N-L, k),name=\"H\")(dropout)\n",
    "    W_H = TimeDistributed(Dense(k,W_regularizer=l2(0.01),name=\"W_ht\"))(H)\n",
    "    W_h = [Lambda(get_X(0), output_shape=(k,))(W_H)]\n",
    "    W_h_eL = [RepeatVector(L)(W_h1[0])]\n",
    "    \n",
    "    ## merge\n",
    "    merge = merge([W_h1_eL[0], W_Y],mode='sum')\n",
    "    M = [Activation('tanh')(merge)]\n",
    "    \n",
    "    #) 2) initialize alpha and r \n",
    "    Distributed_Dense_init_weight = ((2.0/np.sqrt(k)) * np.random.rand(k,1)) - (1.0 / np.sqrt(k))\n",
    "    Distributed_Dense_init_bias = ((2.0) * np.random.rand(1,)) - (1.0)\n",
    "    alpha = [Reshape((L, 1), input_shape=(L,))(Activation(\"softmax\")(Flatten()(TimeDistributed(Dense(1, weights=[Distributed_Dense_init_weight, Distributed_Dense_init_bias]), name='alpha1')(M[0]))))]\n",
    "\n",
    "    Join_Y_alpha = [merge([Y, alpha[0]],mode='concat',concat_axis=2)]    \n",
    "    r = [Lambda(get_R, output_shape=(k,),name=\"r1\")(Join_Y_alpha[0])]\n",
    "    \n",
    "    ##############??????????????????????####################\n",
    "    Tan_Wr_init_weight = 2*(1/np.sqrt(k))*np.random.rand(k,k) - (1/np.sqrt(k))\n",
    "    Tan_Wr_init_bias = 2*(1/np.sqrt(k))*np.random.rand(k,) - (1/np.sqrt(k))\n",
    "    Tan_Wr = [Dense(k,W_regularizer=l2(0.01),activation='tanh', name='Tan_Wr1', weights=[Tan_Wr_init_weight, Tan_Wr_init_bias])(r[0])]\n",
    "\n",
    "    Wr_init_weight = 2*(1/np.sqrt(k))*np.random.rand(k,k) - (1/np.sqrt(k))\n",
    "    Wr_init_bias = 2*(1/np.sqrt(k))*np.random.rand(k,) - (1/np.sqrt(k))\n",
    "    Wr = [Dense(k,W_regularizer=l2(0.01), name='Wr1', weights=[Wr_init_weight, Wr_init_bias])(r[0])]\n",
    "    Wr_cross_e = [RepeatVector(L,name=\"Wr_cross_e\")(Wr[0])]\n",
    "    \n",
    "    #3) update\n",
    "\n",
    "    star_r = []\n",
    "\n",
    "    for i in range(2,N-L+1):\n",
    "        f = get_X(i-1)\n",
    "        W_h.append( Lambda(f, output_shape=(k,))(W_H))\n",
    "        W_h_eL.append( RepeatVector(L)(W_h[i-1]) )\n",
    "\n",
    "        Sum_Wh_lp_cross_e_WY.append( merge([Wh_lp_cross_e[i-1], WY, Wr_cross_e[i-2]],mode='sum') )\n",
    "        M.append( Activation('tanh')(  Sum_Wh_lp_cross_e_WY[i-1] ) )\n",
    "        alpha.append( Reshape((L, 1), input_shape=(L,))(Activation(\"softmax\")(Flatten()(TimeDistributed(Dense(1, weights=[Distributed_Dense_init_weight, Distributed_Dense_init_bias]), name='alpha'+str(i))(M[i-1])))) )\n",
    "\n",
    "        Join_Y_alpha.append( merge([Y, alpha[i-1]],mode='concat',concat_axis=2) )\n",
    "        star_r.append( Lambda(get_R, output_shape=(k,),name=\"r\"+str(i))(Join_Y_alpha[i-1]) )\n",
    "        r.append( merge([star_r[i-2], Tan_Wr[i-2]], mode='sum') )\n",
    "\n",
    "        if i != (N-L):\n",
    "            Tan_Wr.append( Dense(k,W_regularizer=l2(0.01),activation='tanh', name='Tan_Wr'+str(i),weights=[Tan_Wr_init_weight, Tan_Wr_init_bias])(r[i-1]) )\n",
    "            Wr.append( Dense(k,W_regularizer=l2(0.01), name='Wr'+str(i),weights=[Wr_init_weight, Wr_init_bias])(r[i-1]) )\n",
    "            Wr_cross_e.append(RepeatVector(L)(Wr[i-1]))        \n",
    "    \n",
    "    # ok! h* = tanh (W*rN + W*hN)\n",
    "    W_rN = Dense(k, W_regularizer=l2(0.01))(r[N-L-1])\n",
    "    W_hN = Dense(k, W_regularizer=l2(0.01))(hN)\n",
    "    merged = merge([W_rN, W_hN], mode='sum')    \n",
    "    h_star = Activation('tanh')(merged)\n",
    "    \n",
    "    \n",
    "    ####################################\n",
    "    #Output\n",
    "    out  = Dense(3, activation='softmax')(h_star)\n",
    "    model = Model(input=[inputs], output=out)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    # plot(model, 'model.png')\n",
    "    # # model.compile(loss={'output':'binary_crossentropy'}, optimizer=Adam())\n",
    "    # model.compile(loss={'output':'categorical_crossentropy'}, optimizer=Adam(options.lr))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(options.lr))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_question(question):\n",
    "    \"\"\"remove non-letters, return a list of words\"\"\"\n",
    "    if type(question) != float:\n",
    "        question =  re.sub('[^a-zA-Z0-9 -]', \" \", question)\n",
    "        return question.lower().split()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def get_synonyms(word):\n",
    "    #synonyms = wordnet.synsets(word)\n",
    "    #return list(set(chain.from_iterable([word.lemma_names() for word in synonyms])))\n",
    "    synonyms = []\n",
    "    for word in w2v.wv.most_similar(word):\n",
    "        synonyms.append(word[0])    \n",
    "    return synonyms\n",
    "\n",
    "def augment_question(question, k=0.2):\n",
    "    if not question:\n",
    "        return question\n",
    "    new_question = []\n",
    "    words_to_change = np.random.choice(question, int(k*len(question)))\n",
    "    for idx, word in enumerate(question):\n",
    "        if word in words_to_change:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms != []:\n",
    "                synonym = np.random.choice(synonyms)\n",
    "                new_question.append(synonym)\n",
    "            else:\n",
    "                new_question.append(word)\n",
    "        else:\n",
    "            new_question.append(word)\n",
    "    return new_question\n",
    "\n",
    "def question_to_vectors_glove(question, length = 20):\n",
    "    vectors = np.zeros((length,300),dtype = \"float32\")    \n",
    "    number_of_words = 0\n",
    "    \n",
    "    for idx, word in enumerate(question):        \n",
    "        if word in glove_words:            \n",
    "            word_idx = glove_words[word]\n",
    "            vector = glove_vectors[word_idx]\n",
    "            vectors[number_of_words] = vector\n",
    "            number_of_words += 1\n",
    "            \n",
    "        if number_of_words == length:\n",
    "            break          \n",
    "        \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embedding_models():\n",
    "    with open(\"data/glove\", 'rb') as f:\n",
    "        glove = pickle.load(f)\n",
    "    glove_words = glove[0]\n",
    "    glove_vectors = normalize(glove[1])\n",
    "    \n",
    "    _fname = \"data/GoogleNews-vectors-negative300.bin\"\n",
    "    w2v = gensim.models.Word2Vec.load_word2vec_format(_fname, binary=True)\n",
    "    return glove_words, glove_vectors, w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_original_data():\n",
    "    train_full = pd.read_csv('data/train.csv')\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    train_full_y = train_full['is_duplicate']\n",
    "    train_full_q1 = train_full['question1']\n",
    "    train_full_q2 = train_full['question2']\n",
    "    test_q1 = test['question1']\n",
    "    test_q2 = test['question2']\n",
    "    train_q1, cv_q1, train_q2, cv_q2, train_y, cv_y = train_test_split(train_full_q1, train_full_q2, train_full_y, test_size = 0.2, random_state=3)\n",
    "    train_q1, train_q2, train_y = shuffle(train_q1, train_q2, train_y, random_state=3)\n",
    "    cv_q1, cv_q2, cv_y = shuffle(cv_q1, cv_q2, cv_y, random_state=3)\n",
    "    return train_q1, train_q2, train_y, cv_q1, cv_q2, cv_y, test_q1, test_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator_glove(questions1, questions2, y=None, aug_prob=0, batch_size=1024):\n",
    "    \n",
    "    sample_size = len(questions1)\n",
    "    batch_slices = [slice(i, i + batch_size) for i in range(0, sample_size, batch_size)]\n",
    "    \n",
    "    while True:\n",
    "        for batch in batch_slices:\n",
    "            batch_vectors1 = []\n",
    "            batch_vectors2 = []\n",
    "            batch_y = y[batch]\n",
    "\n",
    "            for question1 in questions1[batch]:\n",
    "                prob = np.random.rand()                \n",
    "                question1_new = clean_question(question1)\n",
    "                \n",
    "                if prob < aug_prob: \n",
    "                    question1_new = augment_question(question1_new)                   \n",
    "\n",
    "                vector = question_to_vectors_glove(question1_new)\n",
    "                batch_vectors1.append(vector)\n",
    "            \n",
    "            for question2 in questions2[batch]:\n",
    "                prob = np.random.rand()                \n",
    "                question2_new = clean_question(question2)\n",
    "                \n",
    "                if prob < aug_prob: \n",
    "                    question2_new = augment_question(question2_cleaned)                   \n",
    "\n",
    "                vector = question_to_vectors_glove(question2_new)\n",
    "                batch_vectors2.append(vector)         \n",
    "\n",
    "            if y is None:\n",
    "                # test batch without labels\n",
    "                yield [np.array(batch_vectors1), np.array(batch_vectors2)]\n",
    "            else:\n",
    "                yield [np.array(batch_vectors1), np.array(batch_vectors2)], np.array(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_acc(X, Y, vocab, model, opts):\n",
    "    scores = model.predict(X, batch_size=options.batch_size)\n",
    "    prediction = np.zeros(scores.shape)\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = np.argmax(scores[i])\n",
    "        prediction[i][l] = 1.0\n",
    "    assert np.array_equal(np.ones(prediction.shape[0]), np.sum(prediction, axis=1))\n",
    "    plabels = np.argmax(prediction, axis=1)\n",
    "    tlabels = np.argmax(Y, axis=1)\n",
    "    acc = accuracy(tlabels, plabels)\n",
    "    return acc, acc\n",
    "\n",
    "\n",
    "def getConfig(opts):\n",
    "    conf = [opts.xmaxlen,\n",
    "            opts.ymaxlen,\n",
    "            opts.batch_size,\n",
    "            opts.emb,\n",
    "            opts.lr,\n",
    "            opts.samples,\n",
    "            opts.lstm_units,\n",
    "            opts.epochs]\n",
    "    if opts.no_padding:\n",
    "        conf.append(\"no-pad\")\n",
    "    return \"_\".join(map(lambda x: str(x), conf))\n",
    "\n",
    "\n",
    "def save_model(model, wtpath, archpath, mode='yaml'):\n",
    "    if mode == 'yaml':\n",
    "        yaml_string = model.to_yaml()\n",
    "        open(archpath, 'w').write(yaml_string)\n",
    "    else:\n",
    "        with open(archpath, 'w') as f:\n",
    "            f.write(model.to_json())\n",
    "    model.save_weights(wtpath)\n",
    "\n",
    "\n",
    "def load_model(wtpath, archpath, mode='yaml'):\n",
    "    if mode == 'yaml':\n",
    "        model = model_from_yaml(open(archpath).read())  # ,custom_objects={\"MyEmbedding\": MyEmbedding})\n",
    "    else:\n",
    "        with open(archpath) as f:\n",
    "            model = model_from_json(f.read())  # , custom_objects={\"MyEmbedding\": MyEmbedding})\n",
    "    model.load_weights(wtpath)\n",
    "    return model\n",
    "\n",
    "\n",
    "def concat_in_out(X, Y, vocab):\n",
    "    numex = X.shape[0]  # num examples\n",
    "    glue = vocab[\"delimiter\"] * np.ones(numex).reshape(numex, 1)\n",
    "    inp_train = np.concatenate((X, glue, Y), axis=1)\n",
    "    return inp_train\n",
    "\n",
    "\n",
    "def setup_logger(config_str):\n",
    "    logging.basicConfig(level=logging.DEBUG,\n",
    "                        format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n",
    "                        datefmt='%m-%d %H:%M',\n",
    "                        filename=datetime.now().strftime('mylogfile_%H_%M_%d_%m_%Y.log'),\n",
    "                        filemode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_q1, train_q2, train_y, cv_q1, cv_q2, cv_y, test_q1, test_q2 = load_original_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_words, glove_vectors, w2v = load_embedding_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    train = \n",
    "    cv = \n",
    "    test = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
