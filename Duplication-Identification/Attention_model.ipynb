{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation\n",
    "\n",
    "X: input, left question + one unknown thing + right question\n",
    "length N = len_l  + 1 + len_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import zipfile\n",
    "import urllib\n",
    "import pickle\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.callbacks import *\n",
    "# from visualizer import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "#from keras.utils.np_utils import to_categorical, accuracy\n",
    "from keras.layers.core import *\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge, TimeDistributed\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from keras import layers\n",
    "import theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_i(i):  # get element i\n",
    "    return X[:,i,:]\n",
    "\n",
    "def get_Y(X):\n",
    "    return X[:, :20, :]  # get first xmaxlen elem from time dim\n",
    "\n",
    "def get_H(X):\n",
    "    return X[:, 20:, :] # get elements L+1 to N\n",
    "\n",
    "def get_H_n(X):\n",
    "    ans = X[:, -1, :]  # get last element from time dim\n",
    "    return ans\n",
    "\n",
    "def get_R(X):\n",
    "    Y, alpha = X[0], X[1]\n",
    "    ans = K.batch_dot(Y, alpha)\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_attention(sentence_length=20, verbose=False):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1509.06664\n",
    "    Reasoning about Entailment with Neural Attention\n",
    "    Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiský, Phil Blunsom\n",
    "    Submitted on 22 Sep 2015 (v1), last revised 1 Mar 2016\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    inputs = Input(shape=(sentence_length*2+1, 300), name='inputs')\n",
    "    #x = Embedding(output_dim=opts.emb, input_dim=opts.max_features, input_length=N, name='x')(main_input)\n",
    "    drop_out = Dropout(0.1, name='dropout')(inputs)\n",
    "    \n",
    "    lstm_fwd = LSTM(150, return_sequences=True, name='lstm_fwd')(drop_out)\n",
    "    lstm_bwd = LSTM(150, return_sequences=True, go_backwards=True, name='lstm_bwd')(drop_out)\n",
    "    \n",
    "    bilstm = layers.concatenate([lstm_fwd, lstm_bwd], name='bilstm')\n",
    "    drop_out = Dropout(0.1)(bilstm)\n",
    "    \n",
    "    ###################################################\n",
    "    # Attention \n",
    "    # M = tanh(Wy*Y + (Wh*hN)*eL)\n",
    "    Y = Lambda(get_Y, name=\"Y\", output_shape=(sentence_length, 300))(drop_out)\n",
    "    W_Y = TimeDistributed(Dense(300, W_regularizer=l2(0.01)), name=\"weight_Y\")(Y)\n",
    "    \n",
    "    hN = Lambda(get_H_n, output_shape=(300,), name=\"hN\")(drop_out)   \n",
    "    W_hN = Dense(300, W_regularizer=l2(0.01), name=\"W_hN\")(hN)\n",
    "    W_hN_eL = RepeatVector(sentence_length, name=\"W_hN_eL\")(W_hN)   \n",
    "    \n",
    "    merged = layers.add([W_Y, W_hN_eL], name=\"merged\")\n",
    "    M = Activation('tanh', name=\"M\")(merged)\n",
    "    \n",
    "    # alpha = softmax(wT * Mt)\n",
    "    alpha_ = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_\")(M)\n",
    "    flat_alpha = Flatten(name=\"flat_alpha\")(alpha_)\n",
    "    alpha = Dense(sentence_length, activation='softmax', name=\"alpha\")(flat_alpha)\n",
    "    \n",
    "    # r = Y*alphaT\n",
    "    Y_trans = Permute((2, 1), name=\"Y_trans\")(Y)  # of shape (None,300,20)\n",
    "    r_ = merge([Y_trans, alpha], output_shape=(300, 1), name=\"r_\", mode=get_R)\n",
    "    r = Reshape((300,), name=\"r\")(r_)\n",
    "    \n",
    "    # h* = tanh (Wp*r + Wx * hN)\n",
    "    W_r = Dense(300, W_regularizer=l2(0.01))(r)\n",
    "    W_hN = Dense(300, W_regularizer=l2(0.01))(hN)\n",
    "    merged = layers.add([W_r, W_hN])    \n",
    "    h_star = Activation('tanh')(merged)    \n",
    "    \n",
    "    ####################################\n",
    "    #Output\n",
    "    output  = Dense(3, activation='softmax')(h_star)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    # plot(model, 'model.png')\n",
    "    # # model.compile(loss={'output':'binary_crossentropy'}, optimizer=Adam())\n",
    "    # model.compile(loss={'output':'categorical_crossentropy'}, optimizer=Adam(options.lr))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(options.lr))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_question(question):\n",
    "    \"\"\"remove non-letters, return a list of words\"\"\"\n",
    "    if type(question) != float:\n",
    "        question =  re.sub('[^a-zA-Z0-9 -]', \" \", question)\n",
    "        return question.lower().split()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def get_synonyms(word):\n",
    "    #synonyms = wordnet.synsets(word)\n",
    "    #return list(set(chain.from_iterable([word.lemma_names() for word in synonyms])))\n",
    "    synonyms = []\n",
    "    for word in w2v.wv.most_similar(word):\n",
    "        synonyms.append(word[0])    \n",
    "    return synonyms\n",
    "\n",
    "def augment_question(question, k=0.2):\n",
    "    if not question:\n",
    "        return question\n",
    "    new_question = []\n",
    "    words_to_change = np.random.choice(question, int(k*len(question)))\n",
    "    for idx, word in enumerate(question):\n",
    "        if word in words_to_change:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms != []:\n",
    "                synonym = np.random.choice(synonyms)\n",
    "                new_question.append(synonym)\n",
    "            else:\n",
    "                new_question.append(word)\n",
    "        else:\n",
    "            new_question.append(word)\n",
    "    return new_question\n",
    "\n",
    "def question_to_vectors_glove(question, length = 20):\n",
    "    vectors = np.zeros((length,300),dtype = \"float32\")    \n",
    "    number_of_words = 0\n",
    "    \n",
    "    for idx, word in enumerate(question):        \n",
    "        if word in glove_words:            \n",
    "            word_idx = glove_words[word]\n",
    "            vector = glove_vectors[word_idx]\n",
    "            vectors[number_of_words] = vector\n",
    "            number_of_words += 1\n",
    "            \n",
    "        if number_of_words == length:\n",
    "            break          \n",
    "        \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embedding_models():\n",
    "    with open(\"data/glove\", 'rb') as f:\n",
    "        glove = pickle.load(f)\n",
    "    glove_words = glove[0]\n",
    "    glove_vectors = normalize(glove[1])\n",
    "    \n",
    "    _fname = \"data/GoogleNews-vectors-negative300.bin\"\n",
    "    w2v = gensim.models.Word2Vec.load_word2vec_format(_fname, binary=True)\n",
    "    return glove_words, glove_vectors, w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_original_data():\n",
    "    train_full = pd.read_csv('data/train.csv')\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    train_full_y = train_full['is_duplicate']\n",
    "    train_full_q1 = train_full['question1']\n",
    "    train_full_q2 = train_full['question2']\n",
    "    test_q1 = test['question1']\n",
    "    test_q2 = test['question2']\n",
    "    train_q1, cv_q1, train_q2, cv_q2, train_y, cv_y = train_test_split(train_full_q1, train_full_q2, train_full_y, test_size = 0.2, random_state=3)\n",
    "    train_q1, train_q2, train_y = shuffle(train_q1, train_q2, train_y, random_state=3)\n",
    "    cv_q1, cv_q2, cv_y = shuffle(cv_q1, cv_q2, cv_y, random_state=3)\n",
    "    return train_q1, train_q2, train_y, cv_q1, cv_q2, cv_y, test_q1, test_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator_glove(questions1, questions2, y=None, aug_prob=0, batch_size=1024):\n",
    "    \n",
    "    sample_size = len(questions1)\n",
    "    batch_slices = [slice(i, i + batch_size) for i in range(0, sample_size, batch_size)]\n",
    "    \n",
    "    while True:\n",
    "        for batch in batch_slices:\n",
    "            batch_vectors1 = []\n",
    "            batch_vectors2 = []\n",
    "            batch_y = y[batch]\n",
    "\n",
    "            for question1 in questions1[batch]:\n",
    "                prob = np.random.rand()                \n",
    "                question1_new = clean_question(question1)\n",
    "                \n",
    "                if prob < aug_prob: \n",
    "                    question1_new = augment_question(question1_new)                   \n",
    "\n",
    "                vector = question_to_vectors_glove(question1_new)\n",
    "                batch_vectors1.append(vector)\n",
    "            \n",
    "            for question2 in questions2[batch]:\n",
    "                prob = np.random.rand()                \n",
    "                question2_new = clean_question(question2)\n",
    "                \n",
    "                if prob < aug_prob: \n",
    "                    question2_new = augment_question(question2_cleaned)                   \n",
    "\n",
    "                vector = question_to_vectors_glove(question2_new)\n",
    "                batch_vectors2.append(vector)         \n",
    "\n",
    "            if y is None:\n",
    "                # test batch without labels\n",
    "                yield [np.array(batch_vectors1), np.array(batch_vectors2)]\n",
    "            else:\n",
    "                yield [np.array(batch_vectors1), np.array(batch_vectors2)], np.array(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_q1, train_q2, train_y, cv_q1, cv_q2, cv_y, test_q1, test_q2 = load_original_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_words, glove_vectors, w2v = load_embedding_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = build_model_attention()\n",
    "    \n",
    "    earlyStopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=2, mode='auto')\n",
    "    tensorBoard = keras.callbacks.TensorBoard(log_dir='/home/jupyter/jupyter/log')\n",
    "    \n",
    "    model.fit_generator(data_generator_glove(train_q1, train_q2, train_y, aug_prob=0.0, batch_size=batch_size),\n",
    "                        steps_per_epoch=len(train_y) // batch_size, \n",
    "                        epochs=50, \n",
    "                        validation_data=data_generator_glove(cv_q1, cv_q2, cv_y, batch_size=batch_size),\n",
    "                        validation_steps = len(cv_y) // batch_size, \n",
    "                        #verbose=2,\n",
    "                        callbacks=[tensorBoard, earlyStopping])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel/__main__.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_regularizer=<keras.reg...)`\n",
      "/usr/lib/python3.6/site-packages/ipykernel/__main__.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, name=\"W_hN\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/lib/python3.6/site-packages/ipykernel/__main__.py:40: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes to sum over must not contain the batch axis (axes[1]=[0])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7f1857f3d9d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-35e6c16467e0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mearlyStopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtensorBoard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/jupyter/jupyter/log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-8f600a65a749>\u001b[0m in \u001b[0;36mbuild_model_attention\u001b[0;34m(sentence_length, verbose)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# r = Y*alphaT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mY_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Y_trans\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# of shape (None,300,20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mr_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_R\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/.local/lib/python3.6/site-packages/keras/legacy/layers.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(inputs, mode, concat_axis, dot_axes, output_shape, output_mask, arguments, name)\u001b[0m\n\u001b[1;32m    458\u001b[0m                             \u001b[0mnode_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                             \u001b[0mtensor_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m                             name=name)\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/.local/lib/python3.6/site-packages/keras/legacy/layers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, mode, concat_axis, dot_axes, output_shape, output_mask, arguments, node_indices, tensor_indices, name)\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0minput_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbound_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0minput_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbound_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/.local/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/.local/lib/python3.6/site-packages/keras/legacy/layers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'mask'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sum'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ave'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f95cb71e455f>\u001b[0m in \u001b[0;36mget_R\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_R\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/.local/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36mbatch_dot\u001b[0;34m(x, y, axes)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# behaves like tf.batch_matmul as default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatched_tensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/.local/lib/python3.6/site-packages/theano/tensor/basic.py\u001b[0m in \u001b[0;36mbatched_tensordot\u001b[0;34m(x, y, axes)\u001b[0m\n\u001b[1;32m   3644\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mFinally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mcalls\u001b[0m \u001b[0mbatched_dot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcompute\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3645\u001b[0m     \"\"\"\n\u001b[0;32m-> 3646\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tensordot_as_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatched_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/.local/lib/python3.6/site-packages/theano/tensor/basic.py\u001b[0m in \u001b[0;36m_tensordot_as_dot\u001b[0;34m(a, b, axes, dot, batched)\u001b[0m\n\u001b[1;32m   6104\u001b[0m                     \u001b[0;34m'axes to sum over must not contain the batch axis '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6105\u001b[0m                     \u001b[0;34m'(axes[%i]=%s)'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6106\u001b[0;31m                     (i, axes[i]))\n\u001b[0m\u001b[1;32m   6107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6108\u001b[0m         \u001b[0mbatch_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes to sum over must not contain the batch axis (axes[1]=[0])"
     ]
    }
   ],
   "source": [
    "model_attention = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_attention.save(\"model/model_attention.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    batch_size = 10240\n",
    "    return model.predict_generator(data_generator_glove(test_q1, test_q2, batch_size=batch_size),\n",
    "                                   steps=len(test_q1) // batch_size + 1,\n",
    "                                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(model_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from quora import util\n",
    "sub_fname = util.get_submission_filename()\n",
    "submission = util.make_submission(test['test_id'], y_pred)\n",
    "util.save_submission(sub_fname, submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
