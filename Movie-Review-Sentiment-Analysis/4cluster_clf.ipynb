{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import urllib\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import misc\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_root = 'data/'\n",
    "try:\n",
    "    os.mkdir(data_root)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "model_root = 'model/'\n",
    "try:\n",
    "    os.mkdir(model_root)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_root,'train_x_clean.pkl'), 'rb') as f:\n",
    "    train_x_clean = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(data_root,'train_y_clean.pkl'), 'rb') as f:\n",
    "    train_y_clean = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(data_root,'validation_x_clean.pkl'), 'rb') as f:\n",
    "    validation_x_clean = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(data_root,'validation_y_clean.pkl'), 'rb') as f:\n",
    "    validation_y_clean = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(data_root,'test_x_clean.pkl'), 'rb') as f:\n",
    "    test_x_clean = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(data_root,'test_y_clean.pkl'), 'rb') as f:\n",
    "    test_y_clean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(model_root, 'model_w2v1.pkl'), 'rb') as f:\n",
    "    model_w2v1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train k-means clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "def train_k_means1(model_w2v):\n",
    "    start = time.time() # Start time\n",
    "\n",
    "    # Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "    # average of 5 words per cluster\n",
    "    word_vectors = model_w2v.wv.syn0\n",
    "    num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    model = KMeans( n_clusters = num_clusters )\n",
    "    idx = model.fit_predict( word_vectors )\n",
    "\n",
    "    # Get the end time and print how long the process took\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    # Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "    # a cluster number  \n",
    "    \n",
    "    word_centroid_map = dict(zip(model_w2v.wv.index2word, idx ))\n",
    "    \n",
    "    print \"Time taken for K Means clustering: \", elapsed, \"seconds.\"\n",
    "    return model, word_centroid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  623.678175926 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_k_means = train_k_means1(model_w2v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_kmeans1 = train_k_means[0]\n",
    "word_centroid_map1 = train_k_means[1]                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(model_root,'word_centroid_map1.pkl'), 'wb') as f:\n",
    "    pickle.dump(word_centroid_map1, f)\n",
    "    \n",
    "with open(os.path.join(model_root,'model_kmeans1.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_kmeans1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16490\n"
     ]
    }
   ],
   "source": [
    "print len(word_centroid_map1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Represent reviews with clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# testing case\n",
    "review_cleaned = train_x_clean[1000]\n",
    "with open(os.path.join(model_root, 'word_centroid_map1'), 'rb') as f:\n",
    "    word_centroid_map1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def review_centroids(wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:        \n",
    "        if word in word_centroid_map:\n",
    "            #print word\n",
    "            index = word_centroid_map[word]\n",
    "            #print index\n",
    "            bag_of_centroids[index] += 1\n",
    "            #print bag_of_centroids[index]\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3298,)\n"
     ]
    }
   ],
   "source": [
    "print review_centroids(review_cleaned, word_centroid_map1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dataset_centroids(dataset, word_centroid_map):\n",
    "    new_dataset = []\n",
    "    for review in dataset:\n",
    "        new_review = review_centroids(review, word_centroid_map)\n",
    "        new_dataset.append(new_review)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x_clusters = dataset_centroids(train_x_clean, word_centroid_map1)\n",
    "validation_x_clusters = dataset_centroids(validation_x_clean, word_centroid_map1)\n",
    "test_x_clusters = dataset_centroids(test_x_clean, word_centroid_map1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_root,'train_x_clusters.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_x_clusters, f)\n",
    "    \n",
    "with open(os.path.join(data_root,'validation_x_clusters.pkl'), 'wb') as f:\n",
    "    pickle.dump(validation_x_clusters, f)\n",
    "\n",
    "with open(os.path.join(data_root,'test_x_clusters.pkl'), 'wb') as f:\n",
    "    pickle.dump(test_x_clusters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rd_clf = RandomForestClassifier(n_estimators = 100, random_state=3)\n",
    "rd_clf = rd_clf.fit(train_x_clusters, train_y_clean)\n",
    "accuracy_validation = rd_clf.score(validation_x_clusters, validation_y_clean)\n",
    "#test_validation = rd_clf.score(test_features_bw, test_labels)\n",
    "print accuracy_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8564\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf = lr_clf.fit(train_x_clusters, train_y_clean)\n",
    "accuracy_validation = lr_clf.score(validation_x_clusters, validation_y_clean)\n",
    "print accuracy_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_clf = LinearSVC()\n",
    "svm_clf = svm_clf.fit(train_x_clusters, train_y_clean)\n",
    "accuracy_validation = svm_clf.score(validation_x_clusters, validation_y_clean)\n",
    "print accuracy_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy\n",
      "random forest:  0.8472\n",
      "logistic regression:  0.86008\n",
      "svm:  0.85296\n"
     ]
    }
   ],
   "source": [
    "# performance on test set\n",
    "        \n",
    "print \"Accuracy\"\n",
    "print \"random forest: \", rd_clf.score(test_x_clusters, test_y_clean)\n",
    "print \"logistic regression: \", lr_clf.score(test_x_clusters, test_y_clean)\n",
    "print \"svm: \", svm_clf.score(test_x_clusters, test_y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
