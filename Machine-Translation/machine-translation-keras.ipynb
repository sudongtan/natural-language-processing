{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation\n",
    "starter code: https://github.com/udacity/aind2-nlp-capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Bidirectional, RepeatVector\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with codecs.open(input_file, \"r\", \"utf-8\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = load_data('data/small_vocab_en')\n",
    "french_sentences = load_data('data/small_vocab_fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    tokenized_data = tokenizer.texts_to_sequences(x)\n",
    "    return tokenized_data, tokenizer\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    padder = pad_sequences(x, maxlen=length, padding='post')\n",
    "    return padder\n",
    "\n",
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- English sentences preprocessed: (137861, 15)\n",
      "- English vocabulary size: 199\n",
      "- French sentences preprocessed: (137861, 21, 1)\n",
      "- French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "print('- English sentences preprocessed: {}'.format(preproc_english_sentences.shape))\n",
    "print('- English vocabulary size: {}'.format(len(english_tokenizer.word_index)))\n",
    "print('- French sentences preprocessed: {}'.format(preproc_french_sentences.shape))\n",
    "print('- French vocabulary size: {}'.format(len(french_tokenizer.word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # build the layers\n",
    "    learning_rate = .005\n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    #print(inputs.shape)\n",
    "    layers = Embedding(english_vocab_size, english_vocab_size, mask_zero=False)(inputs)\n",
    "    #print(layers.shape)\n",
    "    layers = Bidirectional(GRU(256, dropout=0.5, recurrent_dropout=0.5))(layers)\n",
    "    layers = RepeatVector(output_sequence_length)(layers)\n",
    "    layers = Bidirectional(GRU(256, dropout=0.5, recurrent_dropout=0.5,\n",
    "                               return_sequences=True))(layers)\n",
    "    layers = TimeDistributed(Dense(4 * french_vocab_size, activation='relu'))(layers)\n",
    "    layers = TimeDistributed(Dense(2 * french_vocab_size, activation='relu'))(layers)\n",
    "    outputs = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(layers)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=sparse_categorical_crossentropy,\n",
    "        optimizer=Adam(learning_rate),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137861, 21)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 21, 200)           40000     \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 512)               701952    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 21, 512)           1181184   \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 1380)          707940    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 21, 690)           952890    \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 21, 345)           238395    \n",
      "=================================================================\n",
      "Total params: 3,822,361\n",
      "Trainable params: 3,822,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "print(tmp_x.shape)\n",
    "\n",
    "# Train the neural network\n",
    "model = build_model(tmp_x.shape, preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index) + 1,\n",
    "    len(french_tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 499s 5ms/step - loss: 9.4636 - acc: 0.4044 - val_loss: 9.5217 - val_acc: 0.4093\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 496s 5ms/step - loss: 9.5389 - acc: 0.4082 - val_loss: 9.5217 - val_acc: 0.4093\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 497s 5ms/step - loss: 7.3420 - acc: 0.4001 - val_loss: 3.0058 - val_acc: 0.4093\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 496s 5ms/step - loss: 2.7239 - acc: 0.4421 - val_loss: 2.3150 - val_acc: 0.4665\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 496s 5ms/step - loss: 2.2131 - acc: 0.4755 - val_loss: 2.0400 - val_acc: 0.4933\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 496s 4ms/step - loss: 2.0292 - acc: 0.4952 - val_loss: 1.8612 - val_acc: 0.5243\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 496s 5ms/step - loss: 1.9534 - acc: 0.5047 - val_loss: 1.8513 - val_acc: 0.5065\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 496s 5ms/step - loss: 1.8586 - acc: 0.5206 - val_loss: 1.6787 - val_acc: 0.5550\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 496s 5ms/step - loss: 1.7833 - acc: 0.5352 - val_loss: 1.6131 - val_acc: 0.5868\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 496s 5ms/step - loss: 1.7005 - acc: 0.5526 - val_loss: 1.5667 - val_acc: 0.5865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fac7c48bb00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tmp_x, preproc_french_sentences, batch_size=1024, \n",
    "                    epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey est jamais parfois en en et il il il est est en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(logits_to_text(model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17, 23,  1,  8, 67,  4, 39,  7,  3,  1, 55,  2, 44,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
